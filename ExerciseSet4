## 4.6.1

```{r}
flights %>%
  mutate(tot_delay = arr_delay + dep_delay) %>%
  group_by(dest) %>%
  summarize(avg_delay = mean(tot_delay, na.rm = TRUE)) %>%
  left_join(select(airports, faa, lon, lat), c("dest" = "faa")) %>%
  ggplot(aes(lon, lat, colour = avg_delay)) +
  borders("state") +
  geom_point(size = 2, alpha = 0.8) +
  xlim(c(-130, -65)) +
  ylim(c(20, 50)) +
  coord_quickmap() +
  viridis::scale_color_viridis()
```
# 5.2.1
```{r}
# with integer division
mutate(flights,
       dep_time = (dep_time %/% 100) * 60 + (dep_time %% 100),
       sched_dep_time = (sched_dep_time %/% 100) * 60 + (sched_dep_time %% 100))

# with rounding operations
mutate(flights,
       dep_time = 60 * floor(dep_time/100) + (dep_time - floor(dep_time/100) * 100),
       sched_dep_time = 60 * floor(sched_dep_time/100) + (sched_dep_time - floor(sched_dep_time/100) * 100))

```

##5.2.2
```{r}
flights %>% 
  mutate(dep_time = (dep_time %/% 100) * 60 + (dep_time %% 100),
         sched_dep_time = (sched_dep_time %/% 100) * 60 + (sched_dep_time %% 100),
         arr_time = (arr_time %/% 100) * 60 + (arr_time %% 100),
         sched_arr_time = (sched_arr_time %/% 100) * 60 + (sched_arr_time %% 100)) %>%
  transmute((arr_time - dep_time) %% (60*24) - air_time)

```
7.2.6 library(ggplot2)

a. annoying <- tibble(

`1` = 1:10,

`2` = `1` * 2 + rnorm(length(`1`)))

b. annoying %>% ggplot() +

geom_point(mapping = aes(x = `1`, y = `2`))

c. annoying$`3` <- annoying$`2` / annoying$`1`

annoying

d. annoying %>% rename(one = `1`, two = `2`, three = `3`)

20.2.9 1. read_excel("data/survey.xlsx", na = c("", "N/A"), col_types = c("text", "text")) |>

mutate(

n_pets = case_when(

n_pets == "none" ~ "0",

n_pets == "two" ~ "2",

TRUE ~ n_pets

),

n_pets = as.numeric(n_pets)

) 2. read_excel("data/roster.xlsx") |>

fill(group, subgroup) |>

print(n = 12) 3. read_excel("data/sales.xlsx", skip = 3, col_names = c("id", "n")) |>

mutate(brand = if_else(str_detect(id, "Brand"), id, NA)) |>

fill(brand) |>

filter(n != "n") |>

relocate(brand) |>

mutate(

id = as.numeric(id),

n = as.numeric(n)

) |>

print(n = 7)

#20.2.9 (4-6)
#Q4
write_xlsx(bake_sale, path = "data/bake-sale.xlsx")
#Q5
write_xlsx(bake_sale, path = "data/bake-sale.xlsx")
#Q6
#Just like reading from a CSV, information on data type is lost when we read the data back in.
This makes Excel files unreliable for caching interim results as well.
##21.5.10 (1)
#Q1
# distinct() is a dbplyr verb that affects rows. Specifically, it creates subset distinct/ unique rows. 

# 21.5.10 (1-2)

#2. Explain what each of the following SQL queries do and try recreate them using dbplyr.

SELECT *

FROM flights

WHERE dep_delay < arr_delay

# this query retrieves all the flights where the departure delay is less than the arrival delay.

flights |>

filter(

dep_delay < arr_delay

)

SELECT *, distance / (air_time / 60) AS speed

FROM flights

# this query calculates the speed and drops all other values from the table (similar to the keep(none) command)

flights |>

mutate(

speed = distance / (air_time / 60),

.keep = c("none")

) |>

show_query()

#Exercise 7.2.4
library(tidyverse)

#Q2
#According to "https://readr.tidyverse.org/reference/read_delim.html," read_csv() and read_tsv() have common arguments such as col_names, col_types, col_select, lazy, n_max, id, quoted_na, guess_max, locale,etc. 

#Q3
#According to "https://readr.tidyverse.org/reference/read_fwf.html," the most important arguments to read_fwf() includes fwf_empty(), fwf_widths(), fwf_positions(), and fwf_cols(). 
#These arguments allow us to describe the field structure while working with a fixed width file which has compact representation of numeric data.

#Q4
read_csv ("x,y\n1,'a,b'", quote = "'")


#Q5
read_csv("a,b\n1,2,3\n4,5,6")
#There are only two columns (a and b), but each row has three numbers (1,2,3) and (4,5,6)

read_csv("a,b,c\n1,2\n1,2,3,4")
#There are only three columns (a and b). However, the first row has two numbers (1,2), while the second row has four numbers (1,2,3,4)

read_csv("a,b\n\"1")
#There are two columns (a and b). However, the first row only has one number (1), and there is an extra "\"

read_csv("a,b\n1,2\na,b")
#This function can be run. However, the first row contain numbers (1,2), while the second row contains characters (a, b)

read_csv("a;b\n1;3")
#This function uses semicolon to separate fields, but only read_csv2() reads semicolon-separated files

# 22.5.3 (1-3)

library(tidyverse)

library(arrow)

dir.create("data", showWarnings = FALSE)

curl::multi_download(

"https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv",

"/Users/fionaouma/Desktop/PUBPOL 890 R Public Policy Research/Exercise Set 4/seattle-library-checkouts.csv",

resume = TRUE

)

seattle_csv <- open_dataset(

sources = "/Users/fionaouma/Desktop/PUBPOL 890 R Public Policy Research/Exercise Set 4/seattle-library-checkouts.csv",

col_types = schema(ISBN = string()),

format = "csv"

)

pq_path <- "/Users/fionaouma/Desktop/PUBPOL 890 R Public Policy Research/Exercise Set 4/seattle-library-checkouts"

seattle_csv |>

group_by(CheckoutYear) |>

write_dataset(path = pq_path, format = "parquet")

tibble(

files = list.files(pq_path, recursive = TRUE),

size_MB = file.size(file.path(pq_path, files)) / 1024^2

)

#file.remove("/Users/fionaouma/Desktop/PUBPOL 890 R Public Policy Research/Exercise Set 4/seattle-library-checkouts.csv")

seattle_pq <- open_dataset(pq_path)

seattle_pq |> glimpse()

#1. Figure out the most popular book each year.

most_popular <-

seattle_pq |>

filter(

MaterialType == "BOOK"

) |>

group_by(

CheckoutYear, Title

) |>

summarize(

TotalCheckouts = sum(Checkouts)

) |>

arrange(

CheckoutYear, desc(TotalCheckouts), Title

) |>

collect()

most_popular |>

group_by(CheckoutYear) |>

slice_max(TotalCheckouts, n=1) |>

collect()

most_popular

#Result -

# A tibble: 18 × 3

# Groups: CheckoutYear [18]

"CheckoutYear Title TotalCheckouts

<int> <chr> <int>

1 2005 <Unknown Title> 11793

2 2006 <Unknown Title> 15148

3 2007 <Unknown Title> 13312

4 2008 <Unknown Title> 11784

5 2009 <Unknown Title> 10623

6 2010 <Unknown Title> 8200

7 2011 <Unknown Title> 5637

8 2012 <Unknown Title> 3606

9 2013 Where'd you go, Bernadette : a novel / Maria Semple. 3977

10 2014 The goldfinch / Donna Tartt. 2919

11 2015 The girl on the train / Paula Hawkins. 3333

12 2016 The girl on the train / Paula Hawkins. 2727

13 2017 The Underground Railroad : a novel / Colson Whitehead. 3525

14 2018 Educated : a memoir / Tara Westover. 5417

15 2019 Where the crawdads sing / Delia Owens. 6913

16 2020 Such a fun age : a novel / Kiley Reid. 1776

17 2021 The vanishing half / Brit Bennett. 3793

18 2022 The maid : a novel / Nita Prose. 3145"

#2. Which author has the most books in the Seattle library system?

most_books <-

seattle_pq |>

filter(

MaterialType == "BOOK"

) |>

group_by(

Creator

) |>

summarize(

Totalbooks = n_distinct(Title)

) |>

arrange(

desc(Totalbooks), Creator

) |>

collect()

most_books

"Result -

# A tibble: 224,936 × 2

Creator Totalbooks

<chr> <int>

# 1 "" 301065

# 2 Patterson, James, 1947- 422

# 3 Shakespeare, William, 1564-1616 397

# 4 Meadows, Daisy 231

# 5 Steel, Danielle 223

# 6 Stilton, Geronimo 216

# 7 Christie, Agatha, 1890-1976 207

# 8 Yolen, Jane 200

# 9 Roberts, Nora 190

# 10 Rylant, Cynthia 184

# i 224,926 more rows

# i Use `print(n = ...)` to see more rows"

#3. How has checkouts of books vs ebooks changed over the last 10 years?

checkout_trend <-

seattle_pq |>

filter(

MaterialType %in% c("BOOK", "EBOOK"),

CheckoutYear %in% c(2012:2022)

) |>

group_by(

MaterialType, CheckoutYear

) |>

summarize(

TotalCheckouts = sum(Checkouts)

) |>

arrange(

MaterialType, CheckoutYear, desc(TotalCheckouts)

) |>

collect()

checkout_trend |>

pivot_wider(

names_from = MaterialType,

values_from = TotalCheckouts

) |>

collect()

#Result

# A tibble: 11 × 3

"CheckoutYear BOOK EBOOK

<int> <int> <int>

1 2012 3984859 543122

2 2013 4401755 754296

3 2014 4168597 1101645

4 2015 3986983 1329606

5 2016 3892986 1436345

6 2017 3971593 1578697

7 2018 3987569 1860638

8 2019 3931688 2130439

9 2020 1241999 2793961

10 2021 2266438 2705503

11 2022 2431502 2265598"
